{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d76806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and set up auto-reload\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../src\"))  # adjust if the notebook runs elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_prompt\n",
    "from Autonomous_Learning_Agent.prompts import clarify_with_user_instructions\n",
    "show_prompt(clarify_with_user_instructions,\"clarify_with_user_instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50677023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%writefile ../src/Autonomous_Learning_Agent/state_scope.py\n",
    "import operator\n",
    "from typing_extensions import Optional,Annotated,List,Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class AgentInputState(MessagesState):\n",
    "    \"\"\"Input state for the full agent - only contains messages from user input.\"\"\"\n",
    "    pass\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"\n",
    "    Main state for the full multi-agent research system.\n",
    "    \n",
    "    Extends MessagesState with additional fields for research coordination.\n",
    "    Note: Some fields are duplicated across different state classes for proper\n",
    "    state management between subgraphs and the main workflow.\n",
    "    \"\"\"\n",
    "    research_brief:Optional[str]\n",
    "    supervisor_messages:Annotated[Sequence[BaseMessage],add_messages]\n",
    "    raw_notes:Annotated[list[str],operator.add]=[]\n",
    "    notes:Annotated[list[str],operator.add]=[]\n",
    "    final_report:str\n",
    "\n",
    "# structured output schema\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    \"\"\"Schema for user clarification decision and questions.\"\"\"\n",
    "    need_clarification:bool=Field(\n",
    "        description=\"Whether the user needs to be asked a clarification qustion\"\n",
    "    )\n",
    "    question:str=Field(\n",
    "        description=\" A question to ad the user to clarify the report scope\"\n",
    "    )\n",
    "    verification:str=Field(\n",
    "        description=\"verify message that will start research after user provided the necessary information\"\n",
    "    )\n",
    "class ResearchQuestion(BaseModel):\n",
    "    \"\"\"Schema for structured research brief generation.\"\"\"\n",
    "    research_brief:str=Field(\n",
    "        description=\"A research qustion that will be used to guide the research\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../src/Autonomous_Learning_Agent/research_agent_scope.py\n",
    "from datetime import datetime\n",
    "from email import message\n",
    "from urllib import response\n",
    "from langchain_core import messages\n",
    "from typing_extensions import Literal\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage,AIMessage,get_buffer_string\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.types import Command\n",
    "from Autonomous_Learning_Agent.prompts import clarify_with_user_instructions,transform_messages_into_research_topic_prompt\n",
    "\n",
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date in human readable format\"\"\"\n",
    "    return datetime.now().strftime(\"%a %b %#d,%Y\")\n",
    "\n",
    "model=init_chat_model(\"google_genai:models/gemini-flash-lite-latest\")\n",
    "\n",
    "def clarify_with_user(state: AgentState) -> Command[Literal[\"write_research_brief\", \"__end__\"]]:\n",
    "    \"\"\"\n",
    "    Determine if the user's request contains sufficient information to proceed with research.\n",
    "    \n",
    "    Uses structured output to make deterministic decisions and avoid hallucination.\n",
    "    Routes to either research brief generation or ends with a clarification question.\n",
    "    \"\"\"\n",
    "    # Set up structured output model\n",
    "    structured_output_model = model.with_structured_output(ClarifyWithUser)\n",
    "\n",
    "    # Invoke the model with clarification instructions\n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=clarify_with_user_instructions.format(\n",
    "            messages=get_buffer_string(messages=state[\"messages\"]), \n",
    "            date=get_today_str()\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Route based on clarification need\n",
    "    if response.need_clarification:\n",
    "        return Command(\n",
    "            goto=END, \n",
    "            update={\"messages\": [AIMessage(content=response.question)]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"write_research_brief\", \n",
    "            update={\"messages\": [AIMessage(content=response.verification)]}\n",
    "        )\n",
    "\n",
    "def write_research_brief(state: AgentState):\n",
    "    \"\"\"\n",
    "    Transform the conversation history into a comprehensive research brief.\n",
    "    \n",
    "    Uses structured output to ensure the brief follows the required format\n",
    "    and contains all necessary details for effective research.\n",
    "    \"\"\"\n",
    "    # Set up structured output model\n",
    "    structured_output_model = model.with_structured_output(ResearchQuestion)\n",
    "    \n",
    "    # Generate research brief from conversation history\n",
    "    response =structured_output_model.invoke([\n",
    "        HumanMessage(content=transform_messages_into_research_topic_prompt.format(\n",
    "            messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "            date=get_today_str()\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Update state with generated research brief and pass it to the supervisor\n",
    "    return {\n",
    "        \"research_brief\": response.research_brief,\n",
    "        \"supervisor_messages\": [HumanMessage(content=f\"{response.research_brief}.\")]\n",
    "    }\n",
    "deep_research_builder=StateGraph(AgentState,input_schema=AgentInputState)\n",
    "deep_research_builder.add_node(\"clarify_with_user\",clarify_with_user)\n",
    "deep_research_builder.add_node(\"write_research_brief\",write_research_brief)\n",
    "\n",
    "deep_research_builder.add_edge(START,\"clarify_with_user\")\n",
    "deep_research_builder.add_edge(\"write_research_brief\",END)\n",
    "# scope_research=deep_research_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf7fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with in-memory checkpointer to test in notebook\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "scope = deep_research_builder.compile(checkpointer=checkpointer)\n",
    "display(Image(scope.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "# from utils import format_mess/ages\n",
    "from langchain_core.messages import HumanMessage\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result = scope.invoke({\"messages\": [HumanMessage(content=\"I want to learn about python.\")]}, config=thread)\n",
    "from utils import format_messages\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = scope.invoke({\"messages\": [HumanMessage(content=\"python fundamentals.\")]}, config=thread)\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf5111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ecad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result=scope.invoke({\"messages\":[HumanMessage(content=\" beginer \")]},config=thread)\n",
    "# format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.markdown import Markdown\n",
    "Markdown(result[\"research_brief\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a76b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda63356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
